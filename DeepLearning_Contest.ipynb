{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AASF + DSC IIITM Gwalior Presents 'Intro to Deep Learning & Kaggle' Session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi !\n",
    "#### Welcome! This notebook is for the contest organized with the above session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions: \n",
    "\n",
    "1. Clone this Github repo to run this notebook.\n",
    "\n",
    "2. Run this notebook. You CAN use any platform ie **Kaggle, Colab etc...** to run the notebook.\n",
    "\n",
    "3.  The notebook is well documented with all the necessay instructions provided in between. Kindly carefully read those instructions.   \n",
    "\n",
    "\n",
    "**ANY SUBMISSION VIOLATING THE INSTRUCTIONS IS SUBJECT TO DISQUALIFICATION.**\n",
    "\n",
    "#### 4. ALL THE SUBMISSIONS WILL BE JUDGED FOR THEIR ACCURACY, CODE QUALITY etc. THE DECISION OF THE JUDGES SHALL BE FINAL & ABIDING ON ALL THE PARTICIPANTS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed5ad155cf9e33cb85db4f93adaab5cdea950c18"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c178c2e36286c15411205c1f0b8ad603e9435c2"
   },
   "source": [
    "## CONTENTS ::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98bd9332124df7df3b27447a926e433022d9c3c7"
   },
   "source": [
    "[ **1 ) Importing Various Modules**](#content1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fcd7dde6c712afa3f833320ff7638addb5c6a72"
   },
   "source": [
    "[ **2 ) Preparing the Data**](#content2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17605176a5dbee7a8d0e9f2bc7c9a9fd0ebd37b1"
   },
   "source": [
    "[ **3 ) Modelling**](#content3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd37b03966854cb4f4576ed34b99e4acd02c8b3f"
   },
   "source": [
    "[ **4 ) Evaluating the Model Performance**](#content4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "892a9fe20cb70273b84d120b69e8d8cef65c7e80"
   },
   "source": [
    "<a id=\"content1\"></a>\n",
    "## 1 ) Importing Various Modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell is for importing the various modules required for coding. \n",
    "\n",
    "#### Some initial modules are loaded for you. If you require any other module, go ahead and import it !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "743b574dc8902f9376b0293208139192ed5717d1",
    "id": "z19bVm7o9zeu",
    "outputId": "e6607658-c349-4b88-b29b-bf08b229daa7"
   },
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    " \n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#model selection and pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# keras \n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "# miscellaneous packages\n",
    "import cv2                  \n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import random as rn\n",
    "from random import shuffle  \n",
    "from zipfile import ZipFile\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c24f32b5ef905f9c4df2a020be4419a77c80237e"
   },
   "source": [
    "<a id=\"content2\"></a>\n",
    "## 2 ) Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section, you will write the code to read and prepare the data. The dataset used for this contest is the [Flowers Recognition Dataset](https://www.kaggle.com/alxmamaev/flowers-recognition) on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8721fdb61bab5e515c8f5489876a97059a2b94f"
   },
   "source": [
    "## 2.1) The Image Size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the cell below, the image size to be used is specified.\n",
    "\n",
    "### For a fair judegment all the submissions need to use the same image size. Therefore, for this contest, the image size is fixed to 128.\n",
    "\n",
    "### Any submission that uses an image size other than 128 is subject to disqualification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Reading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following cell, provide the paths to the individual flower directories within the dataset.\n",
    "\n",
    "#### Currently the paths are according to the naming on Kaggle. So if you import the data in a noteboook on Kaggle or run this notebook on Kaggle and import the [dataset](https://www.kaggle.com/alxmamaev/flowers-recognition), the cell should work absolutely fine !!!\n",
    "\n",
    "#### If you are training on some other platform, like Google Colab or any other , specify the corresponding paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "54f440d5e3da322372601d59e6224a8fcd89c5f8",
    "id": "abZS8dPk9ze1"
   },
   "outputs": [],
   "source": [
    "FLOWER_DAISY_DIR='../input/flowers-recognition/flowers/daisy'\n",
    "FLOWER_SUNFLOWER_DIR='../input/flowers-recognition/flowers/sunflower'\n",
    "FLOWER_TULIP_DIR='../input/flowers-recognition/flowers/tulip'\n",
    "FLOWER_DANDI_DIR='../input/flowers-recognition/flowers/dandelion'\n",
    "FLOWER_ROSE_DIR='../input/flowers-recognition/flowers/rose'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Reading the images and extracting the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell below creates 2 lists, one for the images and one for the labels. These lists will later be populated to hold the images and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[] # This array will hold all the images.\n",
    "\n",
    "Y=[] # This list shall hold the labels for all the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell below has a function to read the data. Complete the function so that it correctly reads the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(flower_type,DIR):\n",
    "    for img in tqdm(os.listdir(DIR)):\n",
    "        label=flower_type  # Assigning the label to this sample.\n",
    "        path = os.path.join(DIR,img) # The path to the image.\n",
    "        _, ftype = os.path.splitext(path)\n",
    "        if ftype == \".jpg\":# Just checking the file extension to be '.jpeg'\n",
    "            \n",
    "           #---------------------------------- X -----------------------------------#\n",
    "           # Given the path to an image, read the image using OpenCV and resize it to have shape (128,128,3).\n",
    "            \n",
    "            \n",
    "            img =  cv2.imread(path)\n",
    "            img = cv2.resize(img , (128 , 128 ))\n",
    "           #---------------------------------- X ------------------------------------#\n",
    "            \n",
    "            # Append the image and the label to the respective lists.\n",
    "            X.append(np.array(img)) \n",
    "            Y.append(str(label))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following lines of code, the above function has been called with appropriate parameters for each flower type.\n",
    "\n",
    "#### Once you have completed the above function, just run these cells to read and prepare the data. !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "46034734b447b127ce033705f44a01e7806a8990",
    "id": "9hn_RjL29ze_",
    "outputId": "40853035-6f1c-4bf4-b2f1-708e7269d2de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 769/769 [00:02<00:00, 329.34it/s]\n"
     ]
    }
   ],
   "source": [
    "make_train_data('Daisy',FLOWER_DAISY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "1909dd2bc123cec910d8214898129ab8b622e62f",
    "id": "6XZpRkLK9zfC",
    "outputId": "2a95e409-446f-47e7-8856-686520e5dc05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 734/734 [00:02<00:00, 278.10it/s]\n"
     ]
    }
   ],
   "source": [
    "make_train_data('Sunflower',FLOWER_SUNFLOWER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "5f65e9627dc53b8cb245bc3babd0ab99a7ad3fd2",
    "id": "YvJMmnNx9zfH",
    "outputId": "d0b62619-ed0e-4581-a143-64e48c0bad4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 984/984 [00:03<00:00, 309.90it/s]\n"
     ]
    }
   ],
   "source": [
    "make_train_data('Tulip',FLOWER_TULIP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "fef985e030d09df0762031cd2ec4b009a1f7711a",
    "id": "KgYj8x-H9zfL",
    "outputId": "0b132777-9fc4-4197-88e3-0facfea80a99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1055/1055 [00:03<00:00, 301.90it/s]\n"
     ]
    }
   ],
   "source": [
    "make_train_data('Dandelion',FLOWER_DANDI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "96a7e6070fe5a74624eeb12a86f609480e158cd1",
    "id": "3mtQH6Vg9zfQ",
    "outputId": "50f204ba-53ab-49ee-aa75-b2bf62e2bcd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 784/784 [00:02<00:00, 310.97it/s]\n"
     ]
    }
   ],
   "source": [
    "make_train_data('Rose',FLOWER_ROSE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images is:  4323\n",
      "The number of labels is:  4323\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of images is: \",len(X))\n",
    "print(\"The number of labels is: \",len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Daisy',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " 'Sunflower',\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now X holds the images and Y holds the labels\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77ec9e39e572f004918ea99a6866ee3a06c68324"
   },
   "source": [
    "## 2.2 ) One Hot Encoding the Labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you had been following since, you know that we have got a list named 'Y' that holds the labels.\n",
    "\n",
    "#### In the following cell, write the code to One Hot encode the labels.\n",
    "\n",
    "### Note that the variable name of the One Hot Encoded labels should still be 'Y'. to proceed. Else you may have to change some portion of the code to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "46155da810657d35899fedf3f055a0cf3f04ec6e",
    "id": "_Gug0CHU9zfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------- X -----------------------------------#\n",
    "# Given the set of labels Y, write code to One Hot Encode it. \n",
    "# Store the one hot encoded labels in variable named 'Y' only.\n",
    "         \n",
    "  \n",
    "   \n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "\n",
    "print(np.unique(Y))\n",
    "#---------------------------------- X ------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Normalize the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cell, write code to normalize the images. If you had followed along, you know that we have a list 'X' that holds the images. Write code to normalize the images in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------- X -----------------------------------#\n",
    "# Given the images  in the list 'X', normalize them. Store the normalized images in variable named 'X' only.\n",
    "\n",
    "X=np.array(X)\n",
    "X= X/255.0\n",
    "\n",
    "# dividing it by max pixel value gets us the normalized value\n",
    "#---------------------------------- X ------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4323"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "901fb55ff34f9e6cc2f35538e7814d53cf31a086"
   },
   "source": [
    "## 2.4) Splitting into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have the normalized images in 'X' and the corresponding labels in 'Y'. \n",
    "\n",
    "#### Use scikit learn [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create train and validation sets.\n",
    "\n",
    "### Again, to be consistent across all the submissions, all the submissions have to use the same test size and random state. \n",
    "\n",
    "### Use test_size=0.25 and random state=42.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "75d23f1232fc974cb4a3dc1b966864bea390e184",
    "id": "4xogXfvm9zfg"
   },
   "outputs": [],
   "source": [
    "#---------------------------------- X -----------------------------------#\n",
    "# Given 'X' ie the images and 'Y' ie the labels, create the train and the validation sets. \n",
    "\n",
    "x_train,x_test,y_train,y_test= train_test_split(X , Y , test_size = 0.25 , random_state = 42)\n",
    "\n",
    "#---------------------------------- X ------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8bce257283715279bf531de0b940a9280cd52373"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f543b578e003e334416835db76b61c21a3391ba1"
   },
   "source": [
    "<a id=\"content3\"></a>\n",
    "## 3 ) Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section, you will actually build the model. \n",
    "\n",
    "### A sample model has been provided in the cell below to start but note that all the submissions ARE free to experiment with the model architecture. \n",
    "\n",
    "### You can freely play with the hyper-paramters such as **learning rate, etc..** and with other parameters such as the batch size, epochs, activation functions and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7993c26fee12209d0fe31f0f034c7328b81571fc"
   },
   "source": [
    "## 3.1 ) Building the ConvNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "01ac04f51fd20947335ec6aced2fd9828debe945",
    "id": "2a2cTLEt9zfm"
   },
   "outputs": [],
   "source": [
    "# A Sample CNN model.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same',strides = (1,1) , activation ='relu', input_shape = (IMG_SIZE,IMG_SIZE,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#---------------------------------- X -----------------------------------#\n",
    " \n",
    "# Write your code here. You can add/remove the layers as u wish.\n",
    "model.add(Conv2D(filters = 64 , kernel_size = (3,3) , strides = (1,1) , activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "## adding the third layer\n",
    "\n",
    "model.add(Conv2D(filters = 96 , kernel_size = (3,3) , strides = (1,1) , activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "## trying with adding another layer\n",
    "\n",
    "model.add(Conv2D(filters = 96 , kernel_size = (3,3) , strides = (1,1) , activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "## adding the final layer\n",
    "\n",
    "model.add(Conv2D(filters = 96 , kernel_size = (3,3) , strides = (1,1) , activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#---------------------------------- X ------------------------------------#\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512 , activation = \"relu\"))\n",
    "model.add(Dense(5, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e75515599edc31ca8c8e4299ea62d259fd6c788"
   },
   "source": [
    "## 3.2 ) The Batch size and the no. of Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "aa6c41f404a017054eb3d309dabdf48bc8822cef",
    "id": "ubXKjfti9zfq"
   },
   "outputs": [],
   "source": [
    "# ------------ You can freely experiment with these parameters-----------------\n",
    "batch_size=128\n",
    "epochs=25\n",
    "\n",
    "## this line reduces the learning rate if the val_accuracy parameter value is reduced in epochs\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduced_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e266b7ed910717ee2d68391b59e4bcde6f930e5c"
   },
   "source": [
    "## 3.3 ) Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you understand the contents of the cell below, go ahead  and experiment with it !!!!\n",
    "\n",
    "#### Else just leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "358f41a7c04bd65219782b439c9f073c1668af7b",
    "id": "F5YP1q3wc6Ck"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f8e6bf822742f85e1733a7de806d738f7453a34"
   },
   "source": [
    "## 3.4 ) Compiling the Keras Model & Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are free to use a different optimizer and different loss function and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "172eb8e295a26fb76f338814798c73568456fd7f",
    "id": "d3TzLswd9zft"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "fca74452122226685c34009fa718e0b62d3bdc78",
    "id": "-BO2A3xD9zfw",
    "outputId": "0bb66247-b0f5-405b-e98a-b5b1ac58ad85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 62, 62, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 29, 29, 96)        55392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 14, 14, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 12, 12, 96)        83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 6, 6, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 96)          83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 2, 2, 96)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               197120    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 440,549\n",
      "Trainable params: 440,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5fff9cd1c60d202293ec992d31548eb63604d9ba"
   },
   "source": [
    "## 3.5 ) Fitting on the Training set and making predcitons on the Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't change anything in this cell. The cell should work fine!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "2736cb961572eaaa843417199d543b7016ed608a",
    "id": "YNVSuB6P9zf0",
    "outputId": "465a2fe5-64ec-4443-9cd1-a58574c62ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "25/25 [==============================] - 58s 2s/step - loss: 1.4098 - accuracy: 0.3696 - val_loss: 1.2422 - val_accuracy: 0.4468\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 1.1916 - accuracy: 0.4929 - val_loss: 1.1709 - val_accuracy: 0.5134\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 59s 2s/step - loss: 1.1176 - accuracy: 0.5360 - val_loss: 1.2190 - val_accuracy: 0.5097\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 1.0697 - accuracy: 0.5584 - val_loss: 1.1493 - val_accuracy: 0.5467\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.9990 - accuracy: 0.5970 - val_loss: 1.0572 - val_accuracy: 0.5735\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.9476 - accuracy: 0.6256 - val_loss: 1.0063 - val_accuracy: 0.5985\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 61s 2s/step - loss: 0.9150 - accuracy: 0.6407 - val_loss: 0.8920 - val_accuracy: 0.6494\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.8877 - accuracy: 0.6564 - val_loss: 0.8994 - val_accuracy: 0.6411\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.8621 - accuracy: 0.6602 - val_loss: 0.9537 - val_accuracy: 0.6290\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.8568 - accuracy: 0.6580 - val_loss: 0.8292 - val_accuracy: 0.6633\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.8118 - accuracy: 0.6830 - val_loss: 0.8530 - val_accuracy: 0.6660\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 56s 2s/step - loss: 0.8124 - accuracy: 0.6920 - val_loss: 0.7900 - val_accuracy: 0.6975\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 56s 2s/step - loss: 0.7690 - accuracy: 0.7049 - val_loss: 0.7869 - val_accuracy: 0.6938\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.7364 - accuracy: 0.7245 - val_loss: 0.7529 - val_accuracy: 0.7058\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.7355 - accuracy: 0.7225 - val_loss: 0.8170 - val_accuracy: 0.6772\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.7400 - accuracy: 0.7136 - val_loss: 0.7574 - val_accuracy: 0.7114\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 61s 2s/step - loss: 0.7087 - accuracy: 0.7293 - val_loss: 0.7045 - val_accuracy: 0.7410\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.6871 - accuracy: 0.7444 - val_loss: 0.7920 - val_accuracy: 0.7049\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 58s 2s/step - loss: 0.6806 - accuracy: 0.7431 - val_loss: 0.7819 - val_accuracy: 0.7197\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.6728 - accuracy: 0.7463 - val_loss: 0.7057 - val_accuracy: 0.7447\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.6431 - accuracy: 0.7531 - val_loss: 0.7032 - val_accuracy: 0.7456\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 58s 2s/step - loss: 0.6264 - accuracy: 0.7669 - val_loss: 0.7332 - val_accuracy: 0.7428\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.6438 - accuracy: 0.7498 - val_loss: 0.6462 - val_accuracy: 0.7734\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 57s 2s/step - loss: 0.6203 - accuracy: 0.7742 - val_loss: 0.7807 - val_accuracy: 0.7058\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 58s 2s/step - loss: 0.6012 - accuracy: 0.7848 - val_loss: 0.6292 - val_accuracy: 0.7771\n"
     ]
    }
   ],
   "source": [
    "History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (x_test,y_test),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d6faaa5dbd6b70d73f4469eaeb9e19938194cd4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e0f2de580cd5eadacda79a06250b3b188987004"
   },
   "source": [
    "<a id=\"content4\"></a>\n",
    "## 4 ) Evaluating the Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section is to evaluate the model that you have build. \n",
    "\n",
    "### Just run the following two cells that plot the 'Model Loss vs Epochs' and the 'Model Accuracy vs Epochs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fddff49b7df1420982bd3a05a63dd38b721e743",
    "id": "5zyO2Sdj9zf2",
    "outputId": "2fdeb1ee-3dc6-45de-eba2-6b672e537ab1"
   },
   "outputs": [],
   "source": [
    "plt.plot(History.history['loss'])\n",
    "plt.plot(History.history['val_loss'])\n",
    "plt.title('Model Loss vs Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22dc277aaaeb3f016bfaa19c1abd612442af49e8",
    "id": "N-JDEQGkuQgp",
    "outputId": "4ec51c19-4870-4b35-d739-493666ac9589"
   },
   "outputs": [],
   "source": [
    "plt.plot(History.history['accuracy'])\n",
    "plt.plot(History.history['val_accuracy'])\n",
    "plt.title('Model Accuracy vs Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87527d690141efa73cd11a33cac0780b0b4f8374"
   },
   "source": [
    "#  THE END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8952f9ac2af66841a9c000126de56b44301d3ce"
   },
   "outputs": [],
   "source": [
    "# the predicted accuracy was 77.71%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
