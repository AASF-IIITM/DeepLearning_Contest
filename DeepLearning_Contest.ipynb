{"cells":[{"metadata":{},"cell_type":"markdown","source":"## AASF + DSC IIITM Gwalior Presents 'Intro to Deep Learning & Kaggle' Session."},{"metadata":{},"cell_type":"markdown","source":"### Hi !\n#### Welcome! This notebook is for the contest organized with the above session."},{"metadata":{},"cell_type":"markdown","source":"### Instructions: \n\n1. Clone this Github repo to run this notebook.\n\n2. Run this notebook. You CAN use any platform ie **Kaggle, Colab etc...** to run the notebook.\n\n3.  The notebook is well documented with all the necessay instructions provided in between. Kindly carefully read those instructions.   \n\n\n**ANY SUBMISSION VIOLATING THE INSTRUCTIONS IS SUBJECT TO DISQUALIFICATION.**\n\n#### 4. ALL THE SUBMISSIONS WILL BE JUDGED FOR THEIR ACCURACY, CODE QUALITY etc. THE DECISION OF THE JUDGES SHALL BE FINAL & ABIDING ON ALL THE PARTICIPANTS.\n"},{"metadata":{"_uuid":"ed5ad155cf9e33cb85db4f93adaab5cdea950c18","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c178c2e36286c15411205c1f0b8ad603e9435c2"},"cell_type":"markdown","source":"## CONTENTS ::"},{"metadata":{"_uuid":"98bd9332124df7df3b27447a926e433022d9c3c7"},"cell_type":"markdown","source":"[ **1 ) Importing Various Modules**](#content1)"},{"metadata":{"_uuid":"7fcd7dde6c712afa3f833320ff7638addb5c6a72"},"cell_type":"markdown","source":"[ **2 ) Preparing the Data**](#content2)"},{"metadata":{"_uuid":"17605176a5dbee7a8d0e9f2bc7c9a9fd0ebd37b1"},"cell_type":"markdown","source":"[ **3 ) Modelling**](#content3)"},{"metadata":{"_uuid":"cd37b03966854cb4f4576ed34b99e4acd02c8b3f"},"cell_type":"markdown","source":"[ **4 ) Evaluating the Model Performance**](#content4)"},{"metadata":{"_uuid":"892a9fe20cb70273b84d120b69e8d8cef65c7e80"},"cell_type":"markdown","source":"<a id=\"content1\"></a>\n## 1 ) Importing Various Modules."},{"metadata":{},"cell_type":"markdown","source":"#### The following cell is for importing the various modules required for coding. \n\n#### Some initial modules are loaded for you. If you require any other module, go ahead and import it !!!"},{"metadata":{"_uuid":"743b574dc8902f9376b0293208139192ed5717d1","id":"z19bVm7o9zeu","outputId":"e6607658-c349-4b88-b29b-bf08b229daa7","trusted":true},"cell_type":"code","source":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#model selection and pre-processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# keras \nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\nfrom keras.layers import Dropout, Flatten,Activation\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n\n# miscellaneous packages\nimport cv2   \nfrom tqdm import tqdm\nimport os\nimport random as rn\nfrom random import shuffle\nfrom zipfile import ZipFile\nfrom PIL import Image","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"c24f32b5ef905f9c4df2a020be4419a77c80237e"},"cell_type":"markdown","source":"<a id=\"content2\"></a>\n## 2 ) Preparing the Data"},{"metadata":{},"cell_type":"markdown","source":"#### In this section, you will write the code to read and prepare the data. The dataset used for this contest is the [Flowers Recognition Dataset](https://www.kaggle.com/alxmamaev/flowers-recognition) on Kaggle."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8721fdb61bab5e515c8f5489876a97059a2b94f"},"cell_type":"markdown","source":"## 2.1) The Image Size."},{"metadata":{},"cell_type":"markdown","source":"#### In the cell below, the image size to be used is specified.\n\n### For a fair judegment all the submissions need to use the same image size. Therefore, for this contest, the image size is fixed to 128.\n\n### Any submission that uses an image size other than 128 is subject to disqualification."},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE=128","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2) Reading the data."},{"metadata":{},"cell_type":"markdown","source":"#### In the following cell, provide the paths to the individual flower directories within the dataset.\n\n#### Currently the paths are according to the naming on Kaggle. So if you import the data in a noteboook on Kaggle or run this notebook on Kaggle and import the [dataset](https://www.kaggle.com/alxmamaev/flowers-recognition), the cell should work absolutely fine !!!\n\n#### If you are training on some other platform, like Google Colab or any other , specify the corresponding paths."},{"metadata":{"_uuid":"54f440d5e3da322372601d59e6224a8fcd89c5f8","id":"abZS8dPk9ze1","trusted":true},"cell_type":"code","source":"FLOWER_DAISY_DIR='../input/flowers-recognition/flowers/daisy'\nFLOWER_SUNFLOWER_DIR='../input/flowers-recognition/flowers/sunflower'\nFLOWER_TULIP_DIR='../input/flowers-recognition/flowers/tulip'\nFLOWER_DANDI_DIR='../input/flowers-recognition/flowers/dandelion'\nFLOWER_ROSE_DIR='../input/flowers-recognition/flowers/rose'\n","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3) Reading the images and extracting the labels"},{"metadata":{},"cell_type":"markdown","source":"#### The cell below creates 2 lists, one for the images and one for the labels. These lists will later be populated to hold the images and the corresponding labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"X=[] # This array will hold all the images.\n\nY=[] # This list shall hold the labels for all the images.","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The cell below has a function to read the data. Complete the function so that it correctly reads the images."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_train_data(flower_type,DIR):\n    for img in tqdm(os.listdir(DIR)):\n        label=flower_type  # Assigning the label to this sample.\n        path = os.path.join(DIR,img) # The path to the image.\n        _, ftype = os.path.splitext(path)\n        if ftype == \".jpg\":# Just checking the file extension to be '.jpeg'\n            \n           #---------------------------------- X -----------------------------------#\n           # Given the path to an image, read the image using OpenCV and resize it to have shape (128,128,3).\n            \n            img = cv2.imread(path)\n            img = cv2.resize(img,(128,128))\n           #---------------------------------- X ------------------------------------#\n            \n            # Append the image and the label to the respective lists.\n            X.append(np.array(img)) \n            Y.append(str(label))        ","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### In the following lines of code, the above function has been called with appropriate parameters for each flower type.\n\n#### Once you have completed the above function, just run these cells to read and prepare the data. !!!"},{"metadata":{"_uuid":"46034734b447b127ce033705f44a01e7806a8990","id":"9hn_RjL29ze_","outputId":"40853035-6f1c-4bf4-b2f1-708e7269d2de","trusted":true},"cell_type":"code","source":"make_train_data('Daisy',FLOWER_DAISY_DIR)","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 769/769 [00:04<00:00, 157.33it/s]\n","name":"stderr"}]},{"metadata":{"_uuid":"1909dd2bc123cec910d8214898129ab8b622e62f","id":"6XZpRkLK9zfC","outputId":"2a95e409-446f-47e7-8856-686520e5dc05","trusted":true},"cell_type":"code","source":"make_train_data('Sunflower',FLOWER_SUNFLOWER_DIR)\n","execution_count":10,"outputs":[{"output_type":"stream","text":"100%|██████████| 734/734 [00:05<00:00, 126.91it/s]\n","name":"stderr"}]},{"metadata":{"_uuid":"5f65e9627dc53b8cb245bc3babd0ab99a7ad3fd2","id":"YvJMmnNx9zfH","outputId":"d0b62619-ed0e-4581-a143-64e48c0bad4e","trusted":true},"cell_type":"code","source":"make_train_data('Tulip',FLOWER_TULIP_DIR)","execution_count":11,"outputs":[{"output_type":"stream","text":"100%|██████████| 984/984 [00:06<00:00, 147.45it/s]\n","name":"stderr"}]},{"metadata":{"_uuid":"fef985e030d09df0762031cd2ec4b009a1f7711a","id":"KgYj8x-H9zfL","outputId":"0b132777-9fc4-4197-88e3-0facfea80a99","trusted":true},"cell_type":"code","source":"make_train_data('Dandelion',FLOWER_DANDI_DIR)","execution_count":12,"outputs":[{"output_type":"stream","text":"100%|██████████| 1055/1055 [00:07<00:00, 146.52it/s]\n","name":"stderr"}]},{"metadata":{"_uuid":"96a7e6070fe5a74624eeb12a86f609480e158cd1","id":"3mtQH6Vg9zfQ","outputId":"50f204ba-53ab-49ee-aa75-b2bf62e2bcd2","trusted":true},"cell_type":"code","source":"make_train_data('Rose',FLOWER_ROSE_DIR)","execution_count":13,"outputs":[{"output_type":"stream","text":"100%|██████████| 784/784 [00:05<00:00, 147.48it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of images is: \",len(X))\nprint(\"The number of labels is: \",len(Y))","execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2fbd51806129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of images is: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of labels is: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"]}]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77ec9e39e572f004918ea99a6866ee3a06c68324"},"cell_type":"markdown","source":"## 2.2 ) One Hot Encoding the Labels."},{"metadata":{},"cell_type":"markdown","source":"#### If you had been following since, you know that we have got a list named 'Y' that holds the labels.\n\n#### In the following cell, write the code to One Hot encode the labels.\n\n### Note that the variable name of the One Hot Encoded labels should still be 'Y'. to proceed. Else you may have to change some portion of the code to follow."},{"metadata":{"_uuid":"46155da810657d35899fedf3f055a0cf3f04ec6e","id":"_Gug0CHU9zfe","trusted":true},"cell_type":"code","source":"#---------------------------------- X -----------------------------------#\n# Given the set of labels Y, write code to One Hot Encode it. \n# Store the one hot encoded labels in variable named 'Y' only.\n            \n\n# Write your code here.\n\n# First we use label encoding to convert string categories to numbers\n# Then convert numbers to binary\nle = LabelEncoder()\nY = le.fit_transform(Y)\n\n# to_categorical where total classes = 5\nY = to_categorical(Y,5)\n\n#---------------------------------- X ------------------------------------#","execution_count":14,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3) Normalize the images."},{"metadata":{},"cell_type":"markdown","source":"#### In the next cell, write code to normalize the images. If you had followed along, you know that we have a list 'X' that holds the images. Write code to normalize the images in the list."},{"metadata":{"trusted":true},"cell_type":"code","source":"#---------------------------------- X -----------------------------------#\n# Given the images  in the list 'X', normalize them. Store the normalized images in variable named 'X' only.\n\nX=np.array(X)\nX= X/255\n# Normalizing pixels between 0 and 1.\n\n#---------------------------------- X ------------------------------------#","execution_count":15,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"901fb55ff34f9e6cc2f35538e7814d53cf31a086"},"cell_type":"markdown","source":"## 2.4) Splitting into Training and Validation Sets"},{"metadata":{},"cell_type":"markdown","source":"#### Now we have the normalized images in 'X' and the corresponding labels in 'Y'. \n\n#### Use scikit learn [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create train and validation sets.\n\n### Again, to be consistent across all the submissions, all the submissions have to use the same test size and random state. \n\n### Use test_size=0.25 and random state=42.\n"},{"metadata":{"_uuid":"75d23f1232fc974cb4a3dc1b966864bea390e184","id":"4xogXfvm9zfg","trusted":true},"cell_type":"code","source":"#---------------------------------- X -----------------------------------#\n# Given 'X' ie the images and 'Y' ie the labels, create the train and the validation sets. \n\nx_train,x_test,y_train,y_test= train_test_split(X,Y,test_size=0.25,random_state=42)\n\n# Setting a Random seed value (42)\nnp.random.seed(42)\nrn.seed(42)\n\n#---------------------------------- X ------------------------------------#","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"8bce257283715279bf531de0b940a9280cd52373","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f543b578e003e334416835db76b61c21a3391ba1"},"cell_type":"markdown","source":"<a id=\"content3\"></a>\n## 3 ) Modeling"},{"metadata":{},"cell_type":"markdown","source":"#### In this section, you will actually build the model. \n\n### A sample model has been provided in the cell below to start but note that all the submissions ARE free to experiment with the model architecture. \n\n### You can freely play with the hyper-paramters such as **learning rate, etc..** and with other parameters such as the batch size, epochs, activation functions and others."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7993c26fee12209d0fe31f0f034c7328b81571fc"},"cell_type":"markdown","source":"## 3.1 ) Building the ConvNet Model"},{"metadata":{"_uuid":"01ac04f51fd20947335ec6aced2fd9828debe945","id":"2a2cTLEt9zfm","trusted":false},"cell_type":"code","source":"# A Sample CNN model.\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (IMG_SIZE,IMG_SIZE,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n#---------------------------------- X -----------------------------------#\n \n# Write your code here. You can add/remove the layers as u wish.\n\n#---------------------------------- X ------------------------------------#\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(5, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e75515599edc31ca8c8e4299ea62d259fd6c788"},"cell_type":"markdown","source":"## 3.2 ) The Batch size and the no. of Epochs."},{"metadata":{"_uuid":"aa6c41f404a017054eb3d309dabdf48bc8822cef","id":"ubXKjfti9zfq","trusted":false},"cell_type":"code","source":"# ------------ You can freely experiment with these parameters-----------------\nbatch_size=128\nepochs=10","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e266b7ed910717ee2d68391b59e4bcde6f930e5c"},"cell_type":"markdown","source":"## 3.3 ) Data Augmentation"},{"metadata":{},"cell_type":"markdown","source":"#### If you understand the contents of the cell below, go ahead  and experiment with it !!!!\n\n#### Else just leave it as it is."},{"metadata":{"_uuid":"358f41a7c04bd65219782b439c9f073c1668af7b","id":"F5YP1q3wc6Ck","trusted":false},"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f8e6bf822742f85e1733a7de806d738f7453a34"},"cell_type":"markdown","source":"## 3.4 ) Compiling the Keras Model & Summary"},{"metadata":{},"cell_type":"markdown","source":"### You are free to use a different optimizer and different loss function and metrics."},{"metadata":{"_uuid":"172eb8e295a26fb76f338814798c73568456fd7f","id":"d3TzLswd9zft","trusted":false},"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fca74452122226685c34009fa718e0b62d3bdc78","id":"-BO2A3xD9zfw","outputId":"0bb66247-b0f5-405b-e98a-b5b1ac58ad85","trusted":false},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fff9cd1c60d202293ec992d31548eb63604d9ba"},"cell_type":"markdown","source":"## 3.5 ) Fitting on the Training set and making predcitons on the Validation set"},{"metadata":{},"cell_type":"markdown","source":"### Don't change anything in this cell. The cell should work fine!!!"},{"metadata":{"_uuid":"2736cb961572eaaa843417199d543b7016ed608a","id":"YNVSuB6P9zf0","outputId":"465a2fe5-64ec-4443-9cd1-a58574c62ab3","trusted":false},"cell_type":"code","source":"History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d6faaa5dbd6b70d73f4469eaeb9e19938194cd4","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e0f2de580cd5eadacda79a06250b3b188987004"},"cell_type":"markdown","source":"<a id=\"content4\"></a>\n## 4 ) Evaluating the Model Performance"},{"metadata":{},"cell_type":"markdown","source":"#### This section is to evaluate the model that you have build. \n\n### Just run the following two cells that plot the 'Model Loss vs Epochs' and the 'Model Accuracy vs Epochs'."},{"metadata":{"_uuid":"0fddff49b7df1420982bd3a05a63dd38b721e743","id":"5zyO2Sdj9zf2","outputId":"2fdeb1ee-3dc6-45de-eba2-6b672e537ab1","trusted":false},"cell_type":"code","source":"plt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss vs Epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22dc277aaaeb3f016bfaa19c1abd612442af49e8","id":"N-JDEQGkuQgp","outputId":"4ec51c19-4870-4b35-d739-493666ac9589","trusted":false},"cell_type":"code","source":"plt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.title('Model Accuracy vs Epochs')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87527d690141efa73cd11a33cac0780b0b4f8374"},"cell_type":"markdown","source":"#  THE END."},{"metadata":{"_uuid":"b8952f9ac2af66841a9c000126de56b44301d3ce","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}